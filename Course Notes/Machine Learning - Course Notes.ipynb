{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "## Multivariate Linear Regression\n",
    "### Feature scaling\n",
    "When different features have very different scales, it can pose problems to the use of gradient descent. The ellipses of the objective function become very skinny. Therefore, the convergence of the optimization problem becomes very slow as the gradient direction doesn't always point to the global optimum. In contrast, when the different features have similar scales, the ellipses are close to circles where the gradient direction points to the global optimum, more or less.\n",
    "\n",
    "As a result, when gradient descent is used, it is a good idea to scale the features so they have similar scales. However, when the analytical solution is available in the case of multiple least squares, it is not necessary to scale the features.\n",
    "\n",
    "## Computing Parameters Analytically\n",
    "### Gradient descent vs normal equation\n",
    "Solving the normal equation involves solving for ${(X^TX)}^{-1}$ whose computational complexity is roughly $O(n^3)$. Therefore, for large data sets where $n$ is large (e.g., $n>10,000$), the use of normal equation is slow but gradient descent still works well when $n$ is large.\n",
    "\n",
    "### Invertibility of ${(X^TX)}$\n",
    "This matrix is not invertible (singular) in two scenarios:\n",
    "1. There are linearly dependent features (collinearity).\n",
    "2. There are more features than observations/training examples. (Delete features or use regularization to resolve this.)\n",
    "\n",
    "# Week 3\n",
    "## Classification and Representation\n",
    "### Classification\n",
    "Classification problems cannot be solved using linear regression approach. For example, if we have a 3-class classification problem and the three classes are coded 1, 2 and 3. If we consider the codes as the values of the response variable in the context of linear regression, then we are enforcing that the three classes to be ordered as such, and also that the differences between the neighboring classes are one. In a general classification problem, there is no natural way to convert qualitative response variable to unique quantitative response variable. This is only one problem with using linear regression for classification.\n",
    "\n",
    "### Decision Boundary\n",
    "The decision boundary can be nonlinear if we add nonlinear (e.g., polynomial) terms in the logistic regression. So it can handle data sets that are not linearly separable.\n",
    "\n",
    "## Logistic Regression Model\n",
    "### Simplified Cost Function\n",
    "The need to find a convex cost function (due to the use of the logistic function as the hypothesis) leads to the use of the following cost function:\n",
    "\n",
    "$J(\\theta)=\\frac{1}{m}\\Sigma_{i=1}^m [-y\\log (h_\\theta(x))-(1-y)\\log (1-h_\\theta(x))]$\n",
    "\n",
    "where $h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}$. The cost approaches infinity if the predicted probability is 0 (or 1) but the actual class is 1 (or 0).\n",
    "\n",
    "## Solving the Problem of Overfitting\n",
    "### Regularized Linear Regression\n",
    "$L_2$ regularization of the linear regression (ridge regression) also has an analytical solution for $\\theta$. $\\theta=(X^TX+\\lambda \\mathrm{diag}(0,1,1,...,1))^{-1}X^Ty$. The matrix $X^TX+\\lambda \\mathrm{diag}(0,1,1,...,1)$ is always invertible with a positive $\\lambda$ even when there are fewer traning examples than features. $L_0$ regularization is NP-hard. $L_1$ regularization is lasso regression.\n",
    "\n",
    "# Week 4\n",
    "## Motivations\n",
    "### Nonlinear Hypotheses\n",
    "The motivating example used in this class to prove that classification algorithms like logistic regression doesn't work well is computer vision problem where the computer is asked to identify the object in an image. A small 50x50 image has 2500 pixels, so it's 2500 features if the image is gray scale and 7500 features if the image is RGB. If the decision boundary is nonlinear, then we need to include nonlinear terms of the original features (e.g., polynomial terms) to help us capture that nonlinear boundary, and then the number of features will grow very quickly (e.g., the number of quadratic terms is $O(n^2)$). So algorithms like logistic regression can't handle this kind of problems.\n",
    "\n",
    "## Neural Networks\n",
    "### Model Representation I\n",
    "The function that converts the inputs to the output is called the activation function, which can be a logistic function.\n",
    "\n",
    "# Week 5\n",
    "## Backpropagation in Practice\n",
    "### Gradient Checking\n",
    "Check the approximate gradient to make sure it's similar to the gradient calculated by backprop, but turn this calculation off before training the network because it's very computationally expensive to calculate the approximate gradient. \n",
    "\n",
    "### Random Initialization\n",
    "Initializing all the weights to be zero will lead to the activations of all the hidden units in a given layer to be identical for all times. This is highly redundant and prevents the network from learning any interesting functions. The weights should all be randomly initialized to break the symmetry.\n",
    "\n",
    "### Putting It Together\n",
    "It is reasonable default to have one hidden layer first. If more than one hidden layers are used, have the same number of hidden units in every layer (usually the more the better).\n",
    "\n",
    "Steps for training a neural ne.twork:\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward prop to get $h_\\Theta(x^{(i)})$ for any $x^{(i)}$.\n",
    "3. Implement code to compute cost function $J(\\Theta)$.\n",
    "4. Implement backprop to compute partial derivatives $\\frac{\\partial}{\\partial\\Theta_{jk}^{(l)}}J(\\Theta)$.\n",
    "5. Use gradient checking to compare gradient computed using backprop vs using numerical estimate of gradient. Then disable gradient checking code.\n",
    "6. Use gradient descent of other optimization methods with backprop to try to minimize $J(\\Theta)$ as a function of parameters $\\Theta$.\n",
    "\n",
    "Cost function of neural network is non-convex, so the optimization algorithm can in theory be stuck in local optima. But usually it's not a big problem, the algorithms can find a good local optimum often.\n",
    "\n",
    "# Week 6\n",
    "## Bias vs. Variance\n",
    "### Learning Curves\n",
    "It is a good idea to plot the learning curve while doing a machine learning project by plotting the training and validation/test errors vs the size of the data set. In general, as the size of the data set becomes larger, the training error increases but the validation/test error decreases. For underfitting (high bias), as the data set becomes larger, the training error and validation/test error approach each other but the errors are large. So getting more data is not going to help much. For overfitting (high variance), the training error will always be very small, and there's in general some gap between training and validation/test errors, but the validation/test error will continue to decrease toward the training error, so there's some benefit to get a larger data set.\n",
    "\n",
    "### Deciding What to Do Next Revisited\n",
    "1. Get more training examples: Fixes high variance\n",
    "2. Try smaller sets of features: Fixes high variance\n",
    "3. Try getting additional features: Fixes high bias\n",
    "4. Try adding polynomial features: Fixes high bias\n",
    "5. Try decresing regularization parameter $\\lambda$: Fixes high bias\n",
    "6. Try increasing $\\lambda$: Fixes high variance\n",
    "\n",
    "Using large neural network is more prone to overfitting and we can use regularization to address overfitting. The number of hidden layers can be selected using cross validation.\n",
    "\n",
    "## Building a Spam Classifier\n",
    "### Error Analysis\n",
    "Recommended approach for machine learning problems:\n",
    "1. Start with a **simple** algorithm that you can implement quickly. Implement it and test it on the cross-validation data.\n",
    "2. Plot learning curves to decide if more data, more features, etc. are likely to help.\n",
    "3. Error analysis: Manually examine the exmaples in the cross vlaidation set that the algorithm made errors on. See if you spot any systematic trend in what type of examples it is making errors on.\n",
    "\n",
    "## Handling Skewed Data\n",
    "### Error Metrics for Skewed Classes\n",
    "Suppose we have a data set with 10,000 training examples, among them, 9990 are patients with benign tumors and only 10 have malignant tumors. If we just have a classifier that classifies all patients as having benign tumors, the accuracy of the classifier is 99.9%. But it misses all patients with malignant tumors. This data set has skewed classes (proportion of positive over negative examples is extreme, i.e., close to 0 or 1).\n",
    "\n",
    "Two concepts are defined for skewed classes problems, precision and recall. Precision is the proportion of true positive (actual: 1, predicted: 1) over all predicted positives. Recall is the proportion of true positive over all actual positives. \n",
    "\n",
    "### Trading Off Precision and Recall\n",
    "There is a tradeoff between the two metrics. $F_1$ score is defined as $2\\frac{PR}{P+R}$. The larger, the better. This is equivalent to minimize $\\frac{1}{P}+\\frac{1}{R}$.\n",
    "\n",
    "## Using Large Data Sets\n",
    "### Data for Machine Learning\n",
    "If the features have sufficient information to predict $y$ accurately, then having a large data set helps. A useful test of whether the features have sufficient information is, given the input, can a human expert (which have a large data set from past experience) confidently predict $y$? Suppose we ask a real estate agent to predict house price just given the square footage, then he/she won't be able to predict the price accurately, so having a large data set is not useful.\n",
    "\n",
    "If we have a lot of features and an algorithm that can handle the large number of features (low bias, small training error), and we also have a large training set (low variance, training error close to test error), then the learning algorithm is likely to have high performance.\n",
    "\n",
    "\n",
    "\n",
    "# Week 7\n",
    "## Large Margin Classification\n",
    "### Large Margin Intuition\n",
    "The objective function of SVM is:\n",
    "\n",
    "$\\min C\\Sigma_{i=1}^my^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})+\\frac{1}{2}\\Sigma_{j=1}^n\\theta_j^2$\n",
    "\n",
    "The cost function replaces the log function in logistic regression. In the presence of outliers, the decision boundary might shift depending on the value of $C$. If $C$ is large (under-regularized), then the optimization will try to minimize the first term of the objective funtion, and the decision boundary will shift a lot (overfitting); if $C$ is not so large (with modest regularization), the decision boundary will be more robust.\n",
    "\n",
    "### Mathematics Behind Large Margin Classification\n",
    "Suppose the SVM classifies all the training examples correctly, i.e., the first term of the objective function is zero, then we are left with the second term of the objective function which is essentially $||\\theta||$. The optimal decision boundary is the solution of this optimization problem.\n",
    "\n",
    "$\\min \\frac{1}{2}||\\theta||^2$\n",
    "\n",
    "s.t. $\\theta^Tx^{(i)}=p^{(i)}||\\theta||\\geq 1$ if $y^{(i)}=1$ and $\\theta^Tx^{(i)}=p^{(i)}||\\theta||\\leq -1$ if $y^{(i)}=0$\n",
    "\n",
    "where $p^{(i)}$ The constraints of the optimization problem is the projection of the $i$-th training example (as a vector) onto the $\\theta$ vector. On the decision boundary, $\\theta^Tx=0$, so the $\\theta$ vector is orthogonal to the $x$ vector which is the decision boundary.\n",
    "\n",
    "To minimize $||\\theta||$, the projections need to be large in magnitude. Therefore, any decision boundary that barely separates the two classes won't have large projections, and the optimal decision boundary is the one where the margin is the largest.\n",
    "\n",
    "## Kernels\n",
    "### Kernels I\n",
    "Kernel is basically a similarity function that measures the closeness between a point and a landmark. Gaussian kernel is:\n",
    "\n",
    "$f^{(i)}=exp(-\\frac{||x-l^{(i)}||^2}{\\sigma^2})$\n",
    "\n",
    "where $l^{(i)}$ is a landmark, $\\sigma$ is a parameter to be choosed.\n",
    "\n",
    "### Kernels II\n",
    "A landmark is placed exactly where we have a training example (on the $x$ space). So if we have $m$ training examples, we will have $m$ landmarks, and $m$ kernels where the $i$-th kernel is the similarity of a point to the $i$-th training example. For each training example $x^{(i)}\\in\\mathbb{R}^{n+1}$, one can calculate $m+1$ (including $f_0^{(i)}=1$) kernels denoted as $f^{(i)}\\in\\mathbb{R}^{m+1}$ which are the new features. \n",
    "\n",
    "The optimization problem to be solved is only slightly different:\n",
    "\n",
    "$\\min C\\Sigma_{i=1}^my^{(i)}cost_1(\\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\\theta^Tf^{(i)})+\\frac{1}{2}\\Sigma_{j=1}^m\\theta_j^2$\n",
    "\n",
    "The actual SVM algorithm uses a slightly different regularization term which involves $\\theta^TM\\theta$ where the matrix $M$ depends on the kernel selected. This is for computation efficiency. Think about cases where we have a huge data set, then the number of features is also huge, and the length of the vector $\\theta$ is also huge. \n",
    "\n",
    "Using kernels on logistic regression is very slow. The computation tricks used on SVM that makes it fast doesn't generalize well to other algorithms.\n",
    "\n",
    "Large $\\sigma^2$ allows features $f_i$ to vary more smoothly. This results in higher bias but lower variance. \n",
    "\n",
    "## SVM in Practice\n",
    "### Using An SVM\n",
    "Linear kernel is just no kernel. If we have a large number of features but a small data set, then maybe we can try the linear kernel which already utilizes many features; using a different kernel function might lead to overfitting.\n",
    "\n",
    "If number of features is small but the data set is large, maybe we can try the Gaussian kernel to fit a more complex decision boundary. **Feature scaling is required before using the Gaussian kernel.** We don't want the norm to be dominated by the variables with very large scales.\n",
    "\n",
    "Only kernel functions that satisfy Mercer's Theorem can make sure SVM's optimization to run correctly and does not diverge.\n",
    "\n",
    "- If $n$ is large (many features) and $m$ is small, use logistic regression or SVM with linear kernel. Not enough data to fit complex function.\n",
    "\n",
    "- If $n$ is small, $m$ is intermediate (10-10,000), use SVM with a Gaussian kernel. \n",
    "\n",
    "- If $n$ is small, $m$ is large (50,000+), create/add more features, then use logistic regression or SVM with linear kernel. SVM with Gaussian kernel might be small. Logistic regression and SVM with linear kernel usually have similar performance.\n",
    "\n",
    "Neural network is likely to work well for most of these settings, but may be slower to train. SVM is a convex optimization problem so there's no local optima.\n",
    "\n",
    "# Week 8\n",
    "## Clustering\n",
    "### Optimization Objective\n",
    "The objective function for K-means is:\n",
    "\n",
    "$J(c^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_K)=\\frac{1}{m}\\Sigma_{i=1}^m||x^{(i)}-\\mu_{c^{(i)}}||^2$\n",
    "\n",
    "where $c^{(i)}$ denotes the index of cluster to which example $x^{(i)}$ is currently assigned, $\\mu_{c^{(i)}}$ is the cluster centroid of cluster to which example $x^{(i)}$ has been assigned. This optimization problem has two sets of decision variables: the assignment variables $c^{(i)}$ and the locations of the centroids $\\mu_k$. The implementation algorithm thus consists of two steps in each iteration: the cluster assignment step where the centroid locations are fixed, and the move centroid step where the assignment is fixed.\n",
    "\n",
    "### Random Initialization\n",
    "Randomly pick several training examples as the initial centroids. To avoid local optima, one can repeat random initialization many times, and pick the one with the lowest objective function. Often times, if the number of clusters is $K=2-10$, multiple random initialization helps. If $K$ is very large, multiple random initialization is less likely to help.\n",
    "\n",
    "### Choosing the Number of Clusters\n",
    "The number of clusters can be ambiguous by visual inspection; after all, this is unsupervised learning with no labels.\n",
    "\n",
    "One method to choose the number of clusters is the elbow method where you plot the objective function value with respect to the number of clusters, and find where the diminishing return starts (the elbow), but sometimes the curve is rather smooth and it's hard to determine where that elbow is. If you see the objective function increases with a larger number of clusters (which should not happen if the global optimum is reached), it is possible that the the optimization problem is stuck at a local optima and the optimization problem should be rerun with different initialization.\n",
    "\n",
    "Another method is to use the subject matter knowledge and ask yourself what are you using K-means for, how would different numbers of clusters serve your purpose.\n",
    "\n",
    "## Principal Component Analysis\n",
    "### Principal Component Analysis Problem Formulation\n",
    "The goal of PCA is to find a hyperplane that minimizes the projection errors onto the hyperplane. The comparison between OLS (left) and PCA (right) is shown below.\n",
    "<img src=\"files/PCA is not OLS.png\">\n",
    "\n",
    "### Principal Component Analysis Algorithm\n",
    "Mean normalization (centering) is always done, and feature scaling can be done to bring features to comparable range of values.\n",
    "\n",
    "The first step is to compute the covariance matrix Sigma (positive semidefinite), \n",
    "$\\Sigma=\\frac{1}{m}\\Sigma_{i=1}^m(x^{(i)})(x^{(i)})^T=X^TX/m$\n",
    "\n",
    "then we need to compute the eigenvectors of the convariance matrix. One can use svd to calculate the eigenvectors or the eig function. But svd function is more numerically stable. The column vectors of the U matrix ($\\mathbb{R}^{n\\times n}$) are the eigenvectors. The first k eigenvectors is defined as $U_{reduce}\\in\\mathbb{R}^{n\\times k}$, then the new vectors $z=U_{reduce}^Tx$.\n",
    "\n",
    "Note that matrix $U$ is an orthogonal matrix: $UU^T=U^TU=I$ which leads to $U^T=U^{-1}$.\n",
    "\n",
    "## Applying PCA\n",
    "### Reconstruction from Compressed Representation\n",
    "$x_{approx}=U_{reduce}z$ will reconstruct approximate $x$ from $z$.\n",
    "\n",
    "### Choosing the Number of Principal Components\n",
    "Average squared projection error is $\\frac{1}{m}\\Sigma_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2$.\n",
    "\n",
    "Total variation in the data is $\\frac{1}{m}\\Sigma_{i=1}^m||x^{(i)}||^2$.\n",
    "\n",
    "Typically we want to choose the number of principal components $k$ to be the smallest value such that the average squared projection error is less than 1 percent of the total variation in the data, in other words, we want 99% of the variance to be retained.\n",
    "\n",
    "Variance retained can be easily calculated by using the singular values of the covariance matrix. The $S$ matrix from the svd of the covariance matrix is a diagonal matrix whose diagonal elements are the non-negative singular values $s_{ii}$ where $i\\in\\{1, 2, \\cdots, n\\}$. The variance retained can be calculated as $\\frac{\\Sigma_{i=1}^ks_{ii}}{\\Sigma_{i=1}^ns_{ii}}$ for a given $k$. Therefore, we just need to pick $k$ so that this ratio is greater than 99%.\n",
    "\n",
    "### Advice for Applying PCA\n",
    "It can speed up certain supervised learning problems where the $x$ variable is high dimensional. We can use PCA to project the high dimensional $x$ to the low dimensional $z$ for the training set (not for the entire data set including the cross validation set and test set), then do the supervised learning with $z$ as the new predictors.\n",
    "\n",
    "It is not a good idea to use PCA to prevent overfitting. In this use, the high-dimensional $x$ is mapped to the low-dimensional $z$, because the number of predictors is lower, the chance of overfitting is less. However, this is a bad practice because the dimensionality reduction is done without knowing the $y$ information, i.e., the dimensionality reduction is unsupervised. It is best to use regularization to prevent overfitting.\n",
    "\n",
    "It is also not recommended to always use PCA to reduce dimension before doing the supervised learning. It is recommended to first do the supervised learning using the raw data, if it doesn't work (e.g., very slow), then consider using PCA.\n",
    "\n",
    "# Week 9\n",
    "## Density Estimation\n",
    "### Problem Motivation\n",
    "Determine the probability of seeing a new $x$ and if it's very small given the previously seen data, then it's flagged as an anomaly.\n",
    "\n",
    "### Algorithm\n",
    "1. Choose features that might be indicative of anomalous examples\n",
    "2. Fit Gaussian distribution to each feature, i.e., find the mean and variance of each feature\n",
    "3. Given new training example $x$, assume all features are independently distributed, then calculate the joint probability $p(x)=\\Pi_{j=1}^np(x_j;\\mu_j,\\sigma_j^2)$. If this value is smaller than a threshold, then it's flagged as an anomaly.\n",
    "\n",
    "## Building an Anomaly Detection System\n",
    "### Developing and Evaluating an Anomaly Detection System\n",
    "Suppose we have a labeled data set with 10000 non-anomalous examples and 20 anomalous examples. One way to split the data is to have 6000 non-anomalous examples in the training set, 2000 non-anomalous examples and 10 anomalous examples in the CV set and the rest in the test set.\n",
    "\n",
    "The data set is clearly skewed, so the evaluation metrics shouldn't be the classification accuracy, we can use precision/recall and F1 score. We can use the CV set to choose the threshold $\\epsilon$ and/or pick the set of features.\n",
    "\n",
    "### Anomaly Detection vs. Supervised Learning\n",
    "Use anomaly detection algorithm if:\n",
    "1. Very small number of positive (anomalous) examples.\n",
    "2. Large number of negative (non-anomalous) examples.\n",
    "3. Many different \"types\" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like.\n",
    "4. Future anomalies may look nothing like any of the anomalous examples we've seen so far.\n",
    "\n",
    "Use supervised learning if:\n",
    "1. Large number of positive and negative examples.\n",
    "2. Enough positive examples for algorithm to get a sense of what positive examples are like, future positive examples likely to be similar to ones in training set.\n",
    "\n",
    "For example, for spam email detection, weather prediction, cancer classification, etc., usually we have enough positive examples. For fraud detection, manufacturing monitoring etc., usually we don't have enough positive examples.\n",
    "\n",
    "### Choosing What Features to Use\n",
    "If the histogram of a feature doesn't look Gaussian, we can try transformations of the features.\n",
    "\n",
    "If the probability $p(x)$ is comparable for normal and anomalous examples, then we need to look for additional features.\n",
    "\n",
    "Choose features that might take on unusually large or small values in the event of an anomaly.\n",
    "\n",
    "## Multivariate Gaussian Distribution (Optional)\n",
    "### Multivariate Gaussian Distribution\n",
    "Suppose we have two variables: CPU load and memory use. We have obtained a Gaussian distribution based on the training data for both variables. When a new test example comes in, it has reasonable CPU load and memory use. But if we plot it in 2-D, it's not close to the training examples we've seen. If we use the anomaly detection algorithm based on univariate Gaussian distribution, then we would incorrectly classify it as non-anomalous.\n",
    "\n",
    "So instead of fitting a univariate Gaussian distribution for each feature, we fit a multivariate Gaussian distribution with a mean vector and a covariance matrix for all features altogether. When there's correlation between features, this captures the covariance which the univariate distribution doesn't. The joint probability using multiple univariate distributions is identical to the probability using multivariate distribution if there's no covariance.\n",
    "\n",
    "### Anomaly detection using the Multivariate Gaussian Distribution\n",
    "Original model (multiple univariate Gaussian distributions):\n",
    "1. Need to manually create features to capture anomalies where $x_1$ and $x_2$ take unusual combinations of values such as $x_1/x_2$.\n",
    "2. It is computationally cheaper which means good scaling to large data set.\n",
    "3. It is okay even if training set is small.\n",
    "\n",
    "Multivariate model:\n",
    "1. Automatically captures correlations between features.\n",
    "2. Computationally more expensive (inverse of covariance matrix).\n",
    "3. Must have the number of examples larger than number of features, or else the covariance matrix $\\Sigma$ is non-invertible. Ususally we want $m\\geq 10n$.\n",
    "\n",
    "The covariance matrix has $n^2/2$ elements that need to be determined.\n",
    "\n",
    "If certain features are perfectly multicollinear, covariance matrix is also non-invertible.\n",
    "\n",
    "## Collaborative Filtering\n",
    "### Collaborative Filtering Algorithm\n",
    "When only the response $y$ is available (such as movie rating), and $x$ (such as features that characterize the movies) and $\\theta$ (such as how different people prefer different types of movies) are not available, one can simultaneously solve for $x$ and $\\theta$. The $x$ and $\\theta$ vectors need to be randomly initialized to small values to break the symmetry; otherwise, the different elements within $x$ and $\\theta$ might be identical to each other.\n",
    "\n",
    "## Low Rank Matrix Factorization\n",
    "### Vectorization: Low Rank Matrix Factorization\n",
    "The response matrix $Y\\in\\mathbb{R}^{n_m\\times n_u}$ has movies in rows and users in columns. The feature matrix $X\\in\\mathbb{R}^{n_m\\times n}$ has the movies in rows. The coefficient matrix $\\Theta\\in\\mathbb{R}^{n_u\\times n}$. The predicted movie ratings are $X\\Theta^T$ which is a low rank matrix.\n",
    "\n",
    "Two movies with small norm of the difference between their feature vectors are similar.\n",
    "\n",
    "### Implementation Detail: Mean Normalization\n",
    "If a user doesn't have any movies for ratings yet, then the predicted ratings for this user would be zero for all movies. But it might be reasonable to predict the ratings as the average rating for that movie from other users. \n",
    "\n",
    "One can calculate the average ratings for all movies, and then substract that from the $Y$ matrix to construct a new $Y$. The new $Y$ is then used for training, and the predicted ratings for the user without any ratings yet would be zero. But to get the ratings in the 0-5 scale, one need to add the average rating vector back to the prediction.\n",
    "\n",
    "# Week 10\n",
    "## Gradient Descent with Large Datasets\n",
    "### Learning with Large Datasets\n",
    "For large datasets, computing a gradient involves summation over all the training examples which is very computationally expensive.\n",
    "\n",
    "One might say we can randomly sample a small dataset from the large dataset. This can be helpful in the high-bias cases where the CV error is very close to the training error and increasing the dataset size doesn't improve the CV error a lot. But for the high-variance cases, increasing the dataset size can lead to significant reduction of the CV error, so it is desirable to use a large dataset. We can plot the learning curve (training and CV errors with respect to training set size) to get an idea.\n",
    " \n",
    "### Stochastic Gradient Descent\n",
    "The original implementationof gradient descent is also called the batch gradient descent which involves summation over all the training examples.\n",
    "\n",
    "Steps:\n",
    "1. Randomly shuffle the dataset.\n",
    "2. Loop through the entire dataset, and in each iteration, use only one training example to update the gradient.\n",
    "3. Run step 2 multiple times (1-10). If the dataset is large, maybe once is sufficient.\n",
    "\n",
    "SGD won't converge to the global optimum, instead the final solution will be trapped in a small terminal set. \n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "Somewhere in between the batch gradient descent and stochastic gradient descent. For each iteration, it typically uses 2 to 100 examples. It could be faster than SGD.\n",
    "\n",
    "### SGD Convergence\n",
    "For the batch gradient descent, we can plot the training error as a function of the number of iterations to see how it converges. For SGD, we can compute $cost(\\theta,(x^{(i)},y^{(i)}))=(h_\\theta(x^{(i)})-y^{(i)})^2/2$ before updating $\\theta$ using $(x^{(i)},y^{(i)})$. The reason to do it before updating $\\theta$ using that training example is, after the update, that cost function is expected to be small.\n",
    "\n",
    "With this, to check the convergence of SGD, we can plot the cost function averaged over the last, say, 1000 examples process by the algorithm. We expect this curve to be decreasing, likely in a non-smooth way. To smooth it or sometimes to better see the trend rather than the noise, we can increase the sample size we average over. If it's increasing, the algorithm is diverging, and we can consider using a smaller learning rate.\n",
    "\n",
    "If we want SGD to converge, we can slowly decrease the learning rate over time, e.g., $\\alpha=\\frac{c_1}{Iteration No.+c_2}$. Many people don't use it because two additional parameters need to be determined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {
   "environment": null,
   "summary": "Machine Learning - Course Notes",
   "url": "https://anaconda.org/hooyeh/machine-learning-course-notes"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
