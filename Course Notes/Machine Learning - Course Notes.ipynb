{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "## Multivariate Linear Regression\n",
    "### Feature scaling\n",
    "When different features have very different scales, it can pose problems to the use of gradient descent. The ellipses of the objective function become very skinny. Therefore, the convergence of the optimization problem becomes very slow as the gradient direction doesn't always point to the global optimum. In contrast, when the different features have similar scales, the ellipses are close to circles where the gradient direction points to the global optimum, more or less.\n",
    "\n",
    "As a result, when gradient descent is used, it is a good idea to scale the features so they have similar scales. However, when the analytical solution is available in the case of multiple least squares, it is not necessary to scale the features.\n",
    "\n",
    "## Computing Parameters Analytically\n",
    "### Gradient descent vs normal equation\n",
    "Solving the normal equation involves solving for ${(X^TX)}^{-1}$ whose computational complexity is roughly $O(n^3)$. Therefore, for large data sets where $n$ is large (e.g., $n>10,000$), the use of normal equation is slow but gradient descent still works well when $n$ is large.\n",
    "\n",
    "### Invertibility of ${(X^TX)}$\n",
    "This matrix is not invertible (singular) in two scenarios:\n",
    "1. There are linearly dependent features (collinearity).\n",
    "2. There are more features than observations/training examples. (Delete features or use regularization to resolve this.)\n",
    "\n",
    "# Week 3\n",
    "## Classification and Representation\n",
    "### Classification\n",
    "Classification problems cannot be solved using linear regression approach. For example, if we have a 3-class classification problem and the three classes are coded 1, 2 and 3. If we consider the codes as the values of the response variable in the context of linear regression, then we are enforcing that the three classes to be ordered as such, and also that the differences between the neighboring classes are one. In a general classification problem, there is no natural way to convert qualitative response variable to unique quantitative response variable. This is only one problem with using linear regression for classification.\n",
    "\n",
    "### Decision Boundary\n",
    "The decision boundary can be nonlinear if we add nonlinear (e.g., polynomial) terms in the logistic regression. So it can handle data sets that are not linearly separable.\n",
    "\n",
    "## Logistic Regression Model\n",
    "### Simplified Cost Function\n",
    "The need to find a convex cost function (due to the use of the logistic function as the hypothesis) leads to the use of the following cost function:\n",
    "\n",
    "$J(\\theta)=\\frac{1}{m}\\Sigma_{i=1}^m [-y\\log (h_\\theta(x))-(1-y)\\log (1-h_\\theta(x))]$\n",
    "\n",
    "The cost approaches infinity if the predicted probability is 0 (or 1) but the actual class is 1 (or 0).\n",
    "\n",
    "## Solving the Problem of Overfitting\n",
    "### Regularized Linear Regression\n",
    "$L_2$ regularization of the linear regression (ridge regression) also has an analytical solution for $\\theta$. $\\theta=(X^TX+\\lambda \\mathrm{diag}(0,1,1,...,1))^{-1}X^Ty$. The matrix $X^TX+\\lambda \\mathrm{diag}(0,1,1,...,1)$ is always invertible with a positive $\\lambda$ even when there are fewer traning examples than features. $L_0$ regularization is NP-hard. $L_1$ regularization is lasso regression.\n",
    "\n",
    "# Week 4\n",
    "## Motivations\n",
    "### Nonlinear Hypotheses\n",
    "The motivating example used in this class to prove that classification algorithms like logistic regression doesn't work well is computer vision problem where the computer is asked to identify the object in an image. A small 50x50 image has 2500 pixels, so it's 2500 features if the image is gray scale and 7500 features if the image is RGB. If the decision boundary is nonlinear, then we need to include nonlinear terms of the original features (e.g., polynomial terms) to help us capture that nonlinear boundary, and then the number of features will grow very quickly (e.g., the number of quadratic terms is $O(n^2)$). So algorithms like logistic regression can't handle this kind of problems.\n",
    "\n",
    "## Neural Networks\n",
    "### Model Representation I\n",
    "The function that converts the inputs to the output is called the activation function, which can be a logistic function.\n",
    "\n",
    "# Week 5\n",
    "## Backpropagation in Practice\n",
    "### Gradient Checking\n",
    "Check the approximate gradient to make sure it's similar to the gradient calculated by backprop, but turn this calculation off before training the network because it's very computationally expensive to calculate the approximate gradient. \n",
    "\n",
    "### Random Initialization\n",
    "Initializing all the weights to be zero will lead to the activations of all the hidden units in a given layer to be identical for all times. This is highly redundant and prevents the network from learning any interesting functions. The weights should all be randomly initialized to break the symmetry.\n",
    "\n",
    "### Putting It Together\n",
    "It is reasonable default to have one hidden layer first. If more than one hidden layers are used, have the same number of hidden units in every layer (usually the more the better).\n",
    "\n",
    "Steps for training a neural ne.twork:\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward prop to get $h_\\Theta(x^{(i)})$ for any $x^{(i)}$.\n",
    "3. Implement code to compute cost function $J(\\Theta)$.\n",
    "4. Implement backprop to compute partial derivatives $\\frac{\\partial}{\\partial\\Theta_{jk}^{(l)}}J(\\Theta)$.\n",
    "5. Use gradient checking to compare gradient computed using backprop vs using numerical estimate of gradient. Then disable gradient checking code.\n",
    "6. Use gradient descent of other optimization methods with backprop to try to minimize $J(\\Theta)$ as a function of parameters $\\Theta$.\n",
    "\n",
    "Cost function of neural network is non-convex, so the optimization algorithm can in theory be stuck in local optima. But usually it's not a big problem, the algorithms can find a good local optimum often.\n",
    "\n",
    "# Week 6\n",
    "## Bias vs. Variance\n",
    "### Learning Curves\n",
    "It is a good idea to plot the learning curve while doing a machine learning project by plotting the training and validation/test errors vs the size of the data set. In general, as the size of the data set becomes larger, the training error increases but the validation/test error decreases. For underfitting (high bias), as the data set becomes larger, the training error and validation/test error approach each other but the errors are large. So getting more data is not going to help much. For overfitting (high variance), the training error will always be very small, and there's in general some gap between training and validation/test errors, but the validation/test error will continue to decrease toward the training error, so there's some benefit to get a larger data set.\n",
    "\n",
    "### Deciding What to Do Next Revisited\n",
    "1. Get more training examples: Fixes high variance\n",
    "2. Try smaller sets of features: Fixes high variance\n",
    "3. Try getting additional features: Fixes high bias\n",
    "4. Try adding polynomial features: Fixes high bias\n",
    "5. Try decresing regularization parameter $\\lambda$: Fixes high bias\n",
    "6. Try increasing $\\lambda$: Fixes high variance\n",
    "\n",
    "Using large neural network is more prone to overfitting and we can use regularization to address overfitting. The number of hidden layers can be selected using cross validation.\n",
    "\n",
    "## Building a Spam Classifier\n",
    "### Error Analysis\n",
    "Recommended approach for machine learning problems:\n",
    "1. Start with a **simple** algorithm that you can implement quickly. Implement it and test it on the cross-validation data.\n",
    "2. Plot learning curves to decide if more data, more features, etc. are likely to help.\n",
    "3. Error analysis: Manually examine the exmaples in the cross vlaidation set that the algorithm made errors on. See if you spot any systematic trend in what type of examples it is making errors on.\n",
    "\n",
    "## Handling Skewed Data\n",
    "### Error Metrics for Skewed Classes\n",
    "Suppose we have a data set with 10,000 training examples, among them, 9990 are patients with benign tumors and only 10 have malignant tumors. If we just have a classifier that classifies all patients as having benign tumors, the accuracy of the classifier is 99.9%. But it misses all patients with malignant tumors. This data set has skewed classes (proportion of positive over negative examples is extreme, i.e., close to 0 or 1).\n",
    "\n",
    "Two concepts are defined for skewed classes problems, precision and recall. Precision is the proportion of true positive (actual: 1, predicted: 1) over all predicted positives. Recall is the proportion of true positive over all actual positives. \n",
    "\n",
    "### Trading Off Precision and Recall\n",
    "There is a tradeoff between the two metrics. $F_1$ score is defined as $2\\frac{PR}{P+R}$. The larger, the better. This is equivalent to minimize $\\frac{1}{P}+\\frac{1}{R}$.\n",
    "\n",
    "## Using Large Data Sets\n",
    "### Data for Machine Learning\n",
    "If the features have sufficient information to predict $y$ accurately, then having a large data set helps. A useful test of whether the features have sufficient information is, given the input, can a human expert (which have a large data set from past experience) confidently predict $y$? Suppose we ask a real estate agent to predict house price just given the square footage, then he/she won't be able to predict the price accurately, so having a large data set is not useful.\n",
    "\n",
    "If we have a lot of features and an algorithm that can handle the large number of features (low bias, small training error), and we also have a large training set (low variance, training error close to test error), then the learning algorithm is likely to have high performance.\n",
    "\n",
    "\n",
    "\n",
    "# Week 7\n",
    "## Large Margin Classification\n",
    "### Large Margin Intuition\n",
    "In the presence of outliers, the decision boundary might shift depending on the value of $C$. If $C$ is large (under-regularized), then the optimization will try to minimize the first term of the objective funtion, and the decision boundary will shift a lot (overfitting); if $C$ is not so large (with modest regularization), the decision boundary will be more robust.\n",
    "\n",
    "### Mathematics Behind Large Margin Classification\n",
    "Suppose the SVM classifies all the training examples correctly, i.e., the first term of the objective function is zero, then we are left with the second term of the objective function which is essentially $||\\theta||$. The optimal decision boundary is the solution of this optimization problem.\n",
    "\n",
    "$\\min \\frac{1}{2}||\\theta||^2$\n",
    "\n",
    "s.t. $\\theta^Tx^{(i)}=p^{(i)}||\\theta||\\geq 1$ if $y^{(i)}=1$ and $\\theta^Tx^{(i)}=p^{(i)}||\\theta||\\leq -1$ if $y^{(i)}=0$\n",
    "\n",
    "where $p^{(i)}$ The constraints of the optimization problem is the projection of the $i$-th training example (as a vector) onto the $\\theta$ vector. On the decision boundary, $\\theta^Tx=0$, so the $\\theta$ vector is orthogonal to the $x$ vector which is the decision boundary.\n",
    "\n",
    "To minimize $||\\theta||$, the projections need to be large in magnitude. Therefore, any decision boundary that barely separates the two classes won't have large projections, and the optimal decision boundary is the one where the margin is the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {
   "environment": null,
   "summary": "Machine Learning - Course Notes",
   "url": "https://anaconda.org/hooyeh/machine-learning-course-notes"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
