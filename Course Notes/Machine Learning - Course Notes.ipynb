{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "## Multivariate Linear Regression\n",
    "### Feature scaling\n",
    "When different features have very different scales, it can pose problems to the use of gradient descent. The ellipses of the objective function become very skinny. Therefore, the convergence of the optimization problem becomes very slow as the gradient direction doesn't always point to the global optimum. In contrast, when the different features have similar scales, the ellipses are close to circles where the gradient direction points to the global optimum, more or less.\n",
    "\n",
    "As a result, when gradient descent is used, it is a good idea to scale the features so they have similar scales. However, when the analytical solution is available in the case of multiple least squares, it is not necessary to scale the features.\n",
    "\n",
    "## Computing Parameters Analytically\n",
    "### Gradient descent vs normal equation\n",
    "Solving the normal equation involves solving for ${(X^TX)}^{-1}$ whose computational complexity is roughly $O(n^3)$. Therefore, for large data sets where $n$ is large (e.g., $n>10,000$), the use of normal equation is slow but gradient descent still works well when $n$ is large.\n",
    "\n",
    "### Invertibility of ${(X^TX)}$\n",
    "This matrix is not invertible (singular) in two scenarios:\n",
    "1. There are linearly dependent features (collinearity).\n",
    "2. There are more features than observations/training examples. (Delete features or use regularization to resolve this.)\n",
    "\n",
    "# Week 3\n",
    "## Classification and Representation\n",
    "### Classification\n",
    "Classification problems cannot be solved using linear regression approach. For example, if we have a 3-class classification problem and the three classes are coded 1, 2 and 3. If we consider the codes as the values of the response variable in the context of linear regression, then we are enforcing that the three classes to be ordered as such, and also that the differences between the neighboring classes are one. In a general classification problem, there is no natural way to convert qualitative response variable to unique quantitative response variable. This is only one problem with using linear regression for classification.\n",
    "\n",
    "### Decision Boundary\n",
    "The decision boundary can be nonlinear if we add nonlinear (e.g., polynomial) terms in the logistic regression. So it can handle data sets that are not linearly separable.\n",
    "\n",
    "## Logistic Regression Model\n",
    "### Simplified Cost Function\n",
    "The need to find a convex cost function (due to the use of the logistic function as the hypothesis) leads to the use of the following cost function:\n",
    "\n",
    "$J(\\theta)=\\frac{1}{m}\\Sigma_{i=1}^m [-y\\log (h_\\theta(x))-(1-y)\\log (1-h_\\theta(x))]$\n",
    "\n",
    "The cost approaches infinity if the predicted probability is 0 (or 1) but the actual class is 1 (or 0).\n",
    "\n",
    "## Solving the Problem of Overfitting\n",
    "### Regularized Linear Regression\n",
    "$L_2$ regularization of the linear regression (ridge regression) also has an analytical solution for $\\theta$. $\\theta=(X^TX+\\lambda \\mathrm{diag}(0,1,1,...,1))^{-1}X^Ty$. The matrix $X^TX+\\lambda \\mathrm{diag}(0,1,1,...,1)$ is always invertible with a positive $\\lambda$ even when there are fewer traning examples than features. $L_0$ regularization is NP-hard. $L_1$ regularization is lasso regression.\n",
    "\n",
    "# Week 4\n",
    "## Motivations\n",
    "### Nonlinear Hypotheses\n",
    "The motivating example used in this class to prove that classification algorithms like logistic regression doesn't work well is computer vision problem where the computer is asked to identify the object in an image. A small 50x50 image has 2500 pixels, so it's 2500 features if the image is gray scale and 7500 features if the image is RGB. If the decision boundary is nonlinear, then we need to include nonlinear terms of the original features (e.g., polynomial terms) to help us capture that nonlinear boundary, and then the number of features will grow very quickly (e.g., the number of quadratic terms is $O(n^2)$). So algorithms like logistic regression can't handle this kind of problems.\n",
    "\n",
    "## Neural Networks\n",
    "### Model Representation I\n",
    "The function that converts the inputs to the output is called the activation function, which can be a logistic function.\n",
    "\n",
    "# Week 5\n",
    "## Backpropagation in Practice\n",
    "### Gradient Checking\n",
    "Check the approximate gradient to make sure it's similar to the gradient calculated by backprop, but turn this calculation off before training the network because it's very computationally expensive to calculate the approximate gradient. \n",
    "\n",
    "### Random Initialization\n",
    "Initializing all the weights to be zero will lead to the activations of all the hidden units in a given layer to be identical for all times. This is highly redundant and prevents the network from learning any interesting functions. The weights should all be randomly initialized to break the symmetry.\n",
    "\n",
    "### Putting It Together\n",
    "It is reasonable default to have one hidden layer first. If more than one hidden layers are used, have the same number of hidden units in every layer (usually the more the better).\n",
    "\n",
    "Steps for training a neural ne.twork:\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward prop to get $h_\\Theta(x^{(i)})$ for any $x^{(i)}$.\n",
    "3. Implement code to compute cost function $J(\\Theta)$.\n",
    "4. Implement backprop to compute partial derivatives $\\frac{\\partial}{\\partial\\Theta_{jk}^{(l)}}J(\\Theta)$.\n",
    "5. Use gradient checking to compare gradient computed using backprop vs using numerical estimate of gradient. Then disable gradient checking code.\n",
    "6. Use gradient descent of other optimization methods with backprop to try to minimize $J(\\Theta)$ as a function of parameters $\\Theta$.\n",
    "\n",
    "Cost function of neural network is non-convex, so the optimization algorithm can in theory be stuck in local optima. But usually it's not a big problem, the algorithms can find a good local optimum often.\n",
    "\n",
    "# Week 6\n",
    "## Bias vs. Variance\n",
    "### Learning Curves\n",
    "It is a good idea to plot the learning curve while doing a machine learning project by plotting the training and validation/test errors vs the size of the data set. In general, as the size of the data set becomes larger, the training error increases but the validation/test error decreases. For underfitting (high bias), as the data set becomes larger, the training error and validation/test error approach each other but the errors are large. So getting more data is not going to help much. For overfitting (high variance), the training error will always be very small, and there's in general some gap between training and validation/test errors, but the validation/test error will continue to decrease toward the training error, so there's some benefit to get a larger data set.\n",
    "\n",
    "### Deciding What to Do Next Revisited\n",
    "1. Get more training examples: Fixes high variance\n",
    "2. Try smaller sets of features: Fixes high variance\n",
    "3. Try getting additional features: Fixes high bias\n",
    "4. Try adding polynomial features: Fixes high bias\n",
    "5. Try decresing regularization parameter $\\lambda$: Fixes high bias\n",
    "6. Try increasing $\\lambda$: Fixes high variance\n",
    "\n",
    "Using large neural network is more prone to overfitting and we can use regularization to address overfitting. The number of hidden layers can be selected using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {
   "environment": null,
   "summary": "Machine Learning - Course Notes",
   "url": "https://anaconda.org/hooyeh/machine-learning-course-notes"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
